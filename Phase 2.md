#  **Phase 2: Agentic Code Generation & Basic History Loop**.

**Goal:** To test the core agentic loop mechanics: can the chosen LLM (Gemini 2.0 Flash Thinking) generate code modifications (initially as full files for simplicity) based on visual history to fix a layout issue, and can a basic script manage this loop for a few iterations with minimal manual intervention?

**Estimated Cost:** Low-Moderate (~$5-15 depending on exact token counts per iteration for ~5-6 iterations using a Flash model). Moderate developer time to build the minimal loop framework.

**Steps & Implementation Details:**

1.  **Select Test Case:**
    *   Use the *same small cluster* of 3-5 overlapping labels identified in Phase 1 (from `baseline_us_cat1_5.png`).
    *   Isolate the *data* corresponding to these few points into a small test DataFrame/CSV file (e.g., `test_cluster_data.csv`). This simplifies the plotting script for this phase.

2.  **Adapt Plotting Script (`plotting_script.py`):**
    *   Modify the `baseline_plot.py` script from Phase 0.
    *   It should now load data from `test_cluster_data.csv`.
    *   Crucially, structure it so it can be run repeatedly and its *behavior can be modified by changing its source code*. (No complex parameterization needed, the agent will directly edit the code).
    *   Ensure it saves its output image with a predictable, iteration-specific name (e.g., `output_iteration_N.png`).
    *   Start with the version that produces the initial overlap for the test cluster. Save this as `plotting_script_v0.py`.

3.  **Build Minimal Loop Framework (`agent_loop_minimal.py`):**
    *   This script orchestrates the process.
    *   **Initialization:**
        *   Define the number of iterations (e.g., `NUM_ITERATIONS = 6`).
        *   Load the initial plotting code (`plotting_script_v0.py`) into a string variable `current_code`.
        *   Initialize state variables: `previous_code = None`, `previous_image_path = None`, `previous_llm_modification = None`.
    *   **Loop (Iterate from N=0 to NUM_ITERATIONS-1):**
        *   **a. Save Current Code:** Write the content of `current_code` to a temporary file, e.g., `temp_plotter.py`.
        *   **b. Execute Plotting Script:** Run `temp_plotter.py` using `subprocess.run(['python', 'temp_plotter.py'])`. This should generate `output_iteration_N.png`. Handle potential execution errors minimally.
        *   **c. Prepare LLM Input:**
            *   `current_image_path = f'output_iteration_{N}.png'`
            *   Assemble the prompt and multimodal input for the LLM (see Step 4). Include `current_code` string, `current_image_path`, `previous_image_path` (if N>0), and `previous_llm_modification` string (if N>0).
        *   **d. Call LLM API:** Send the input to the Gemini 2.0 Flash Thinking API. Request that the LLM outputs the *entire modified Python code* for the plotting script.
        *   **e. Process LLM Output:**
            *   Extract the Python code block from the LLM's response. Perform basic validation (e.g., does it look like Python code?).
            *   Store the *raw LLM response string* (which contains the new code) as `current_llm_modification`.
        *   **f. Update State for Next Loop:**
            *   `previous_code = current_code`
            *   `current_code = <extracted_python_code_from_LLM>`
            *   `previous_image_path = current_image_path`
            *   `previous_llm_modification = current_llm_modification`
            *   **(Crucial Logging):** Save the state for each iteration (N, prompt used, LLM response, image path) to a log file for later analysis.
    *   **Cleanup:** Remove temporary files if needed.

4.  **Craft Iterative Prompt for LLM:**
    *   This prompt is used inside the loop (`step 3.c`).
    *   It needs to provide context and clear instructions.
    *   Example Prompt Structure (adapt based on API specifics):
        ```text
        You are an AI agent improving a Python script that generates a map visualization using Matplotlib and Cartopy. Your goal is to fix overlapping labels.

        **Context:**
        *   **Current Plotting Script (`current_code`):**
            ```python
            # [Content of current_code string pasted here]
            ```
        *   **Image from Previous Script (`previous_image`):** [Image data for previous_image_path loaded here, if N>0]
        *   **Modification made by LLM in Previous Step (`previous_llm_modification`):**
            ```
            # [Content of previous_llm_modification string pasted here, if N>0]
            ```
        *   **Image Generated by Current Script (`current_image`):** [Image data for current_image_path loaded here]

        **Task:**
        1.  Analyze the `current_image` to identify ONE instance of overlapping text labels. Compare with `previous_image` (if available) to understand the effect of the last modification.
        2.  Modify the `current_code` to fix the overlap you identified. You might need to adjust the x, y coordinates in `ax.text()` calls or potentially adjust other plotting parameters.
        3.  Output the *entire, complete, modified Python script*. Do not add explanations, comments outside the code, or any text other than the Python code itself, enclosed in markdown triple backticks (```python ... ```).

        **Constraint:** Ensure the output is runnable Python code that uses Matplotlib/Cartopy. Modify only the plotting logic to improve label layout.
        ```

5.  **Execute and Observe:**
    *   Run the `agent_loop_minimal.py` script.
    *   Monitor the console output (add print statements for status).
    *   Observe the generated images (`output_iteration_0.png` through `output_iteration_5.png`).
    *   Check the log file.

6.  **Evaluate:**
    *   Did the loop complete the specified number of iterations?
    *   Review the sequence of generated images: Is there *any* sign of improvement (even if clumsy or introducing new issues)? Did labels move? Did overlaps decrease in some cases?
    *   Review the logs: Did the LLM generate syntactically valid Python code each time? Were the modifications *conceptually* related to label positions? Did the LLM seem to use the history (e.g., not repeat the exact same failed modification)?
    *   **Pass/Fail:** This phase passes if the agent generated code that modified label positions over several iterations, even if the final result isn't perfect. Failure occurs if the agent consistently produces invalid code, doesn't modify labels, or the loop crashes irrecoverably due to LLM errors.

**Expected Outputs:**

1.  A small data file (`test_cluster_data.csv`).
2.  An initial plotting script (`plotting_script_v0.py`) for the cluster.
3.  The main loop script (`agent_loop_minimal.py`).
4.  A sequence of output images (`output_iteration_0.png` to `output_iteration_5.png`).
5.  A log file containing the inputs (code, history) and outputs (LLM response/code) for each iteration.
6.  An engineer assessment of whether the basic loop mechanism functioned and if the LLM made *any* relevant code modifications targeting label layout over the iterations.

This phase focuses squarely on testing the "closed loop" of Image -> LLM Analysis -> Code Generation -> Image, including history passing and automated code updates, albeit on a very small scale.