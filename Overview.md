## **Project Goal:**

*   Verify if an agentic loop, using a multimodal LLM (Gemini 2.0 Flash Thinking) with visual feedback (current image, previous image, code diff), can iteratively generate Python code (Matplotlib/Cartopy) to produce a static hurricane landfall map subset with demonstrably improved label layout (meeting "no severe overlaps" & "readable labels" criteria) compared to a naive baseline, staying within a ~$100 API budget.

**Core Hypothesis:**

*   Providing visual history and code diff context significantly aids the LLM in making effective code modifications for complex visual layout tasks, mitigating the visual-to-code bottleneck.

**Key Assumptions & Constraints:**

*   **LLM:** Gemini 2.0 Flash Thinking (gemini-2.0-flash-thinking-exp-01-21) available via API.
*   **Budget:** ~$100 USD for LLM API calls. Requires mindful usage, especially if using the more expensive Pro model. Flash might be preferred for Phase 3 iterations.
*   **Scope:** Focus on US landfalls initially, potentially just a dense region (like Florida) for the core loop test (Phase 3). No need to replicate the *entire* original map.
*   **Output:** Static PNG image.
*   **Metric:** Qualitative improvement judged against baseline: reduced severe overlaps, readable labels.
*   **Engineer Role:** Setup, oversight, evaluation, potentially minor prompt tuning or manual intervention if the agent gets completely stuck, but primarily observing the automated loop.
*   **Minimalism:** Focus on the *simplest implementation* at each stage to test the core concept. Avoid premature optimization or building robust tooling.

**High-Level Project Phases:**

## **Phase 0: Foundation & Baseline Setup (Low Cost, Dev Time)**

*   **Objective:** Prepare data, environment, and a "worst-case" baseline map.
*   **Key Activities:**
    1.  **Data Prep:** Obtain and parse HURDAT2 data into a simple format (e.g., Pandas DataFrame) with essential columns (lat, lon, name, year, category) for US landfalls (1851-present).
    2.  **Environment Setup:** Standard Python environment with `pandas`, `matplotlib`, `cartopy`, `google-generativeai`.
    3.  **Baseline Script:** Create a script (`baseline_plot.py`) that loads the data and uses Matplotlib/Cartopy to plot *all* US landfall points on a map, adding raw text labels directly at the point coordinates (guaranteed to overlap heavily). Save as `baseline_us.png`.
    4.  **API Connectivity:** Ensure basic connection to the chosen Gemini API is working.

## **Phase 1: Agent Visual Critique POC (Very Low Cost, ~1-2 API Calls)**

*   **Objective:** Verify the LLM can visually identify basic layout problems from the rendered image and *describe* potential fixes.
*   **Key Activities:**
    1.  **Select Test Case:** Identify a small, clearly overlapping cluster of 3-5 labels on `baseline_us.png`.
    2.  **Manual Prompting:** Feed this image snippet to the LLM. Prompt: "Analyze this map snippet. Identify specific label overlaps. Describe in text how you would adjust label positions or lines to fix them (do not write code)."
    3.  **Evaluate Response:** Does the LLM correctly spot the obvious overlaps? Are its text-based suggestions logical? (Engineer judgment).

## **Phase 2: Single-Step Code Generation & Basic History Test (Low-Moderate Cost, ~2-5 API Calls)**

*   **Objective:** Test the LLM's ability to generate a valid *code diff* for a targeted fix, and test the basic loop structure passing history.
*   **Key Activities:**
    1.  **Simple Loop Framework:** Create a basic script (`agent_loop_minimal.py`) to manage the loop state (current code, previous code, previous image, current image, previous diff).
    2.  **Targeted Prompt:** Using the same small cluster from Phase 1, prompt the LLM: "Here is the current script, the previous image, previous diff (if any), and current image. Generate a Python code diff to move the label for [Hurricane X] slightly to resolve the overlap with [Hurricane Y]. Provide only the diff."
    3.  **Apply & Re-render:** Manually verify/apply the diff generated by the LLM. Re-run the script to generate the next image.
    4.  **Iterate Once:** Run the prompt again, this time feeding the *new* state (including the just-generated image and applied diff) to see if the LLM uses the history for its next suggestion.
    5.  **Evaluate:** Did the LLM produce a valid, targeted diff? Did the second iteration show any awareness of the first change?

## **Phase 3: Scaled Agentic Loop Experiment (Highest Cost Phase, Monitor Budget)**

*   **Objective:** Run the loop more automatically on a denser region, potentially adding points incrementally, to assess convergence and layout improvement potential.
*   **Key Activities:**
    1.  **Select Dense Region:** Filter data for a complex area (e.g., Florida).
    2.  **Enhance Loop Script:**
        *   Implement automated diff application (potentially with basic safety checks).
        *   Optionally add logic to plot points incrementally (e.g., add 5-10 new points + labels per iteration).
        *   Optionally integrate `adjustText` as a baseline layout attempt, making the agent's task *refining* its output.
    3.  **Refine Prompt:** Adjust the prompt for the iterative process: "Analyze current image, considering history (prev image/diff). Identify layout issues in the currently plotted points. Generate a code diff to improve layout (e.g., fix overlaps, improve clarity, potentially by adjusting `adjustText` parameters or overriding specific label positions)."
    4.  **Run Iterations:** Execute the loop for a set number of iterations (e.g., 10-20) or until the budget nears its limit. Log all inputs/outputs (prompts, images, diffs, LLM responses). Use Gemini 1.5 Flash here if budget is tight.
    5.  **Observe:** Monitor the process â€“ does the layout visibly improve, stagnate, or degrade? Does the agent get stuck?

## **Phase 4: Evaluation & Hypothesis Assessment (Low Cost, Analysis Time)**

*   **Objective:** Evaluate the results of Phase 3 against the baseline and criteria to determine if the core hypothesis is supported.
*   **Key Activities:**
    1.  **Compare Results:** Visually compare the final map section from Phase 3 against the same section in `baseline_us.png`.
    2.  **Assess Criteria:** Did Phase 3 achieve "no severe overlaps" and "readable labels" significantly better than the baseline? (Engineer judgment).
    3.  **Analyze Logs:** Review agent behavior, common failure modes, successes, and iteration count vs. improvement.
    4.  **Cost Check:** Report final API cost.
    5.  **Conclusion:** Summarize findings. Does the evidence suggest this agentic loop with history is a viable strategy for this task under the tested conditions? What are the limitations and potential next steps?

This plan focuses on getting signal quickly and cheaply at each stage before committing to the more complex and costly Phase 3. It directly tests the core idea of using history to improve the agent's code generation for visual layout.